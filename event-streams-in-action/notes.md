
A company is an organization that generates and responds to a continuous stream of events.

an event is anything that we can observe occurring at a particular point in time. That’s it, fin. Figure 1.1 sets out four example events from four different business sectors.



An event is not any of the following: ·   A description of the ongoing state of something—The day was warm; the car was black; the API client was broken. But “the API client broke at noon on Tuesday” is an event.

A collection of individual events—The Franco-Prussian war involved the Battle of Spicheren, the Siege of Metz, and the Battle of Sedan. But “war was declared between France and Prussia on 19 July 1870” is an event.

Here’s a general rule of thumb: if the thing you are describing can be tied to a specific point in time, chances are that you are describing an event of some kind, even if it needs some verbal gymnastics to represent it.

Now that we have defined what an event is, what is a continuous event stream? Simply put, a continuous event stream is an unterminated succession of individual events, ordered by the point in time at which each event occurred. Figure 1.2 sketches out what a continuous event stream looks like at a high level: you can see a succession of individual events, stepping forward in time. 

We say that the succession of events is unterminated, because of these facts: ·   The start of the stream may predate our observing of the stream. ·   The end of the stream is at some unknown point in the future.

1.2.1   Application-level logging Let’s start with the event stream that almost all backend (and many frontend) developers will be familiar with: application-level logging.

You can see that application-level logging is generally used to record specific events at a point in time. The

You can see that application-level logging is generally used to record specific events at a point in time.

What happens to the log events after they are generated by your application? Best practice says that you write the log events to disk as log files,

So application-level logging is clearly a continuous event stream, albeit one that leans heavily on schemaless messages that are often only human-readable.

1.2.2   Web analytics Let’s move on to another example. If you are a frontend web developer, there’s a good chance that you have embedded JavaScript tags in a website or web app to provide some kind of web or event analytics.

Either way, this code will run for each visitor to the website, generating a continuous stream of events representing each visitor’s set of interactions with the website. These events flow back to Google, where they are stored, processed, and displayed in a variety of reports.

1.2.3   Publish/subscribe messaging Let’s take a slightly lower-level example, but hopefully

1.2.3   Publish/subscribe messaging Let’s take a slightly lower-level example, but hopefully still one that many readers will be familiar with: application messaging, specifically in the publish/subscribe pattern. Publish/subscribe, sometimes shortened to pub/sub,

application logging, web analytics, and publish/subscribe messaging. The terminology may be different, but in all three examples, you can see the same building blocks: a structure or schema of events (even if extremely minimal); a way of generating these events; and a way of collecting and subsequently processing these events.

every digital business should be restructured around a process that does the following: ·   Collects events from disparate source systems ·   Stores them in a unified log ·   Enables data processing applications to operate on these event streams

These two eras bring us up to the present day, and the emerging unified era of data processing. The key innovation in business terms is putting a unified log at the heart of all of our data collection and processing. A unified log is an append-only log to which we write all events generated by our applications. Going further, the unified log has these characteristics: ·   Can be read from at low latency. ·   Is readable by multiple applications simultaneously, with different applications able to consume from the log at their own pace. ·   Holds only a rolling window of events—probably a week or a month’s worth. But we can archive the historic log data in the Hadoop Distributed File System (HDFS) or Amazon Simple Storage Service (S3).

The new architecture is guided by two simple rules: ·   All software systems can and should write their individual continuous event streams to the unified log. Even third-party SaaS vendors can emit events via webhooks and streaming APIs. ·   Unless very low-latency or transactional guarantees are required, software systems should communicate with each other in an uncoupled way through the unified log, not via point-to-point connections.



We have a single version of the truth. Together, the unified log plus Hadoop archive represent our single version of the truth. They contain exactly the same data—our event stream—but they have different time windows of data.

Figure 1.11 Our unified log acts as the glue between all of our software systems. In place of the proliferation of point-to-point connections seen prior, we now have systems reading and writing to the unified log.

We have a few unified log implementations to choose from. We’ll pick Apache Kafka, an open source, self-hosted unified log to get us started. With the scene set, we will code up our simple Java application, start configuring Kafka, and then code the integration between our app and Kafka. This process has a few discrete steps: 1. Defining a simple format for our events 2. Setting up and configuring our unified log 3. Writing events into our unified log 4. Reading events from our unified

We have a few unified log implementations to choose from. We’ll pick Apache Kafka, an open source, self-hosted unified log to get us started. With the scene set, we will code up our simple Java application, start configuring Kafka, and then code the integration between our app and Kafka. This process has a few discrete steps: 1. Defining a simple format for our events 2. Setting up and configuring our unified log 3. Writing events into our unified log 4. Reading events from our unified log

All this talk of a unified log, but what exactly is it? A unified log is an append-only, ordered, distributed log that allows a company to centralize its continuous event streams.

What does it mean that our log is unified? It means that we have a single deployment of this technology in our company (or division or whatever), with multiple applications sending events to it and reading events from it. The Apache Kafka project (Kafka is a unified log) explains it as follows on its homepage (https//kafka.apache.org/): Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization.

Let’s imagine a metropolitan taxi firm that is embracing the unified log wholeheartedly. Several interesting “actors” are involved in this taxi business: ·   Customers booking taxis ·   Taxis generating location, speed, and fuel-consumption data ·   The Dispatch Engine assigning taxis to customer bookings Figure 2.1 demonstrates one possible way of architecting this taxi firm around its new unified log implementation. The three streams share the same unified log; there is no reason for them not to. Applications such as the Dispatch Engine can read from two continuous streams of events and write into another stream.

Figure 2.1 The unified log for our taxi firm contains three streams: a taxi-booking stream, a taxi-monitoring stream, and a taxi-dispatching stream.

Append-only means that new events are appended to the front of the unified log, but existing events are never updated in place after they’re appended. What about deletion? Events are automatically deleted from the unified log when they age beyond a configured time window, but they cannot be deleted in an ad hoc fashion.

Distributed might sound a little confusing: is the log unified, or is it distributed? Actually, it’s both! Distributed and unified are referring to different properties of the log. The log is unified because a single unified log implementation is at the heart of the business, as explained previously in section 2.1.1. The unified log is distributed because it lives across a cluster of individual machines.

Scalability—Having the unified log distributed across a cluster of machines allows us to work with event streams larger than the capacity of any single machine.

Durability—A unified log will replicate all events within the cluster.

To make it easier to work across a cluster of machines, unified logs tend to divide the events in a given event stream into multiple shards (sometimes referred to as partitions); each

To make it easier to work across a cluster of machines, unified logs tend to divide the events in a given event stream into multiple shards (sometimes referred to as partitions); each shard will be replicated to multiple machines for durability,

Ordered means that the unified log gives each event in a shard a sequential ID number (sometimes called the offset) that uniquely identifies each message within the shard. Keeping the ordering restricted to the shard keeps things much simpler—because there is no need to maintain and share a global ordering across the whole cluster.

This ordering gives the unified log much of its power: different applications can each maintain their own cursor position for each shard, telling them which events they have already processed, and thus which events they should process next.

Even though our standard definition of an event as subject-verb-object will be described in section 2.1.7, we can already identify three discrete events in this Viewing through Buying workflow:

1. Shopper views product at time—Occurs every time the shopper views a product,

2. Shopper adds item to cart at time—Occurs whenever the shopper adds one of those products to the shopping basket.

3. Shopper places order at time—Occurs when the shopper checks out, paying for the items in the shopping basket.

Figure 2.6 Our three types of event are flowing through, into the raw-events topic in Kafka. Order events are written to Kafka directly from the website’s backend; the in-browser events of viewing products and adding products to the cart are written to Kafka via an HTTP server that collects events.

It is up to us to define the internal format of our events—a process we call modeling our events.

How should we model our first event, Shopper views product at time?

Figure 2.7 Our shopper (subject) views (verb) a product (direct object) at time (prepositional object).

Let’s go through each of these components in turn: ·   The “shopper” is the sentence’s subject. The subject in a sentence is the entity carrying out the action: Jane views an iPad at midday. ·   “views” is the sentence’s verb, describing the action being done by the subject: “Jane views an iPad at midday.” ·   The “product” being viewed is the direct object, or simply object. This is the entity to which the action is being done: “Jane views an iPad at midday.” ·   The time of the event is, strictly speaking, another object—an indirect object, or prepositional object, where the preposition is “at”: “Jane views an iPad at midday.”

JSON has the attractive property of being easily written and read by both people and machines. Many, if not most, developers setting out to model their company’s continuous event streams will start with JSON.

{   "event": "SHOPPER_VIEWED_PRODUCT",   # A   "shopper": {                         # B     "id": "123",     "name": "Jane",     "ipAddress": "70.46.123.145"   },   "product": {                         # C     "sku": "aapl-001",     "name": "iPad"   },   "timestamp": "2018-10-15T12:01:35Z"  # D }

To look at it another way, our event consists of two pieces of event metadata (namely, the event and the timestamp), and two business entities (the shopper and the product).

We are going to send the event stream generated by Nile into a unified log. For this, we’re going to pick Apache Kafka. Future chapters cover Kafka in much more detail. For now, it’s just important to understand that Kafka is an open source (Apache License 2.0) unified log technology that runs on the JVM.

This time, press Ctrl-C to exit. As a final test, let’s pretend that Nile has a second application that also wants to consume from the raw-events stream. It’s a key property of a unified log technology such as Kafka that we can have multiple applications reading from the same event stream at their own pace.

Fantastic—our second request to read the raw-events topic from the beginning has returned the exact same three events. This helps illustrate the fact that Kafka is serving as a kind of event stream database. Compare this to a pub/sub message queue, where a single subscriber reading messages “pops them off the queue,” and they are gone for good.

At its simplest, event processing involves reading one or more events from an event stream and doing something to those events. That processing operation could be filtering an event from the stream, validating the event against a schema, or enriching the event with additional information. Or we could be processing multiple events at a time, perhaps with a view to reordering them or creating some kind of summary or aggregate of those events.

We’ll keep our stream-processing application simple: we’ll stick to validating Nile’s incoming raw events and enriching the valid events. Enriching means adding interesting extra information to an event.



Figure 3.1 Our original event stream is being processed by four applications, one of which is generating a new event stream of its own. Note that each of our four applications can have a different current offset, or cursor position, in the original stream; this is a feature of unified logs.

The general term for what all of these example applications are doing is event stream processing. We can say that any program or algorithm that understands the time-ordered, append-only nature of a continuous event stream and can consume events from this stream in a meaningful way is able to process this stream.

single-event processing, is straightforward to implement: we read the next event from our continuous event stream and apply some sort of transformation to it. We can apply many transformations, and common ones include the following: ·   Validating the event—Checking, for example, “Does this event contain all the required fields?” ·   Enriching the event—Looking up, for example, “Where is this IP address located?” ·   Filtering the event—Asking, for example, “Is this error critical?”

Figure 3.3. Here the stream processing application is validating, enriching, and filtering an incoming raw stream. Events that make it through the whole transformation are added to our processed stream. Events that fail validation are added to our errors stream. Transformation warnings are added to our warnings stream.

In multiple-event processing, we have to read multiple events from the event stream in order to generate some kind of output. Plenty of algorithms and queries fit into this pattern, including these: ·   Aggregating—Applying aggregate functions such as minimum, maximum, sum, count, or average on multiple events ·   Pattern matching—Looking for patterns or otherwise summarizing the sequence, co-occurrence, or frequency of multiple events ·   Sorting—Reordering events based on a sort key

1. Read the raw-events topic in Kafka 2. Figure out where each shopper is located 3. Write the events, now with the country and city attached, out to another Kafka topic Figure 3.5 illustrates this flow. Figure 3.5 Our first stream-processing app will read events from the raw-events topic in Apache Kafka and write enriched events back to a new topic in Kafka. As our unified log, Kafka is the glue between multiple applications.

A company called MaxMind (www.maxmind.com) provides a free-to-use database that maps IP addresses to geographical location. We can look up each shopper’s IP address in the MaxMind geo-IP database to determine where the shopper is located at that point in time. When we use algorithms or external databases to add extra data points to an event, we typically say that we are enriching the event.

After we have validated our events, we need to identify where each event originated geographically.

So far, we are validating the incoming event and then enriching it. The final step will be to write out the validated, enriched events to a new Kafka topic: enriched-events.

Putting it together, we need to create a stream processing application that does the following: ·   Reads individual events from our Kafka topic raw-events ·   Validates the event’s IP address, sending any validation failures to a dedicated Kafka topic, called bad-events ·   Enriches our validated events with the geographical location of the shopper by using the MaxMind geo-IP database ·   Writes our validated, enriched events to the enriched-events Kafka topic

Figure 3.6 Our single-event processing app is going to read events from the raw-events topic, validate the incoming events, enrich the valid events with the geographical location, and route the enriched events to enriched-events. Any errors will be written out to bad-events.

Read events from our Kafka topic raw-events ·   Validate the events, writing any validation failures to the bad-events Kafka topic ·   Enrich our validated events with the geographical location of the shopper by using the MaxMind geo-IP database ·   Write our validated, enriched events to the enriched-events Kafka topic

There’s quite a lot to take in here. The control flow is perhaps better visualized in a diagram, as shown in figure 3.9. The important thing to understand is that we are looking up the shopper’s IP address in MaxMind, and if it’s found, we are attaching the shopper’s country and city to the outgoing enriched event. If anything goes wrong on the way, we write that error message out to the “bad” topic. 

Figure 3.9. Our single-event processor attempts to enrich the raw event with the geolocation as looked up in the MaxMind database; if anything goes wrong, an error is written out instead.

Check back in the fourth terminal (the console-consumer) and you should see our original three raw events appearing in the enriched-events Kafka topic, but this time with the geolocation data attached—namely, the country and city fields:

This looks great! We are successfully enriching our incoming events, adding useful geographical context to these events for the Nile data scientists.

Remember that our systems administrator wants to receive a warning whenever the troublesome server’s disk reaches 80% full. To support this monitoring, the agent running on the server will need to regularly read filesystem metrics and send those metrics into our unified log for further analysis. We can model these metrics readings as events by using the grammatical structure introduced in chapter 2: ·   Our agent is the subject of the event. ·   Read (“took a reading”) is the verb of the event. ·   Filesystem metrics are the direct object of the event. ·   The reading takes place on our server, a prepositional object. ·   The reading takes place at a specific time, another prepositional object. Putting these together, we can sketch out the event model that we’ll need to assemble, as in figure 4.5. Figure 4.5 Our systems monitoring events involve an agent reading filesystem metrics on a given server at a specific point in time.

Before we can read events from our stream, we need to retrieve what Amazon calls a shard iterator for each shard in the stream. A shard iterator is a slightly abstract concept. You can think of it as something like a short-lived (5-minute) file handle on a given shard. Each shard iterator defines a cursor position for a set of sequential reads from the shard, where the cursor position takes the form of the sequence number of the first event to read. Let’s create a shard iterator now:

Figure 4.9 illustrates the various shard iterator options. Figure 4.9 The four configuration options for a Kinesis shard iterator allow us to start reading a shard of our event stream from various points.

Another thing to stress is that, just as in Kafka, each of these records is still stored in the Kinesis stream, available for other applications to consume. It’s not like the act of reading has “popped” these events off the shard forever.

We require a shard iterator to read events from a single shard in the stream. Think of this as a temporary stream handle defining our cursor position in the stream. ·   We use the shard iterator to read a batch of events from the shard. ·   Along with the batch of events, we receive back the next shard iterator, which we use to read the next batch.

Figure 4.10 illustrates this process. Figure 4.10 After we have retrieved our first shard iterator, each request for records returns a new shard iterator we can use for the next request.

The AWS CLI and boto expose the exact same primitives for stream processing, which is unsurprising, given that the AWS CLI is built on boto! The main difference is that the AWS Python SDK will let us use all the power of Python in our stream processing application.

Figure 4.11 Our monitoring application will read events from our Kinesis stream, check whether the reported disk usage has reached 80%, and raise an alert if so.

Now we need a loop in each thread to handle reading events from each shard via shard iterators. The loop will be broadly the same as that of figure 4.11: ·   Get an initial shard iterator for the given shard ·   Use the shard iterator to read a batch of events from the shard ·   Use the returned next shard iterator to read the next batch of events

Again, this is something that higher-level frameworks like the Kinesis Client Library handle: they allow you to “checkpoint” your progress against a stream by using persistent storage such as Amazon’s DynamoDB.

In chapter 3, we introduced the idea of processing continuous event streams and implemented a simple application that processed individual shopping events from the Nile website. The app we wrote did a few neat things: it read individual events from Kafka, filtered out bad input events, enriched the event with location information, and finally wrote the newly filtered and enriched event back out to Kafka.

We need to maintain some form of state to keep track of shopper behavior across multiple events; this brings with it attendant challenges, such as distributing the processing and the state safely over multiple servers. To meet these challenges, we will introduce stream processing frameworks at a high level.

Figure 5.1 On the left side, Shopper A has added two products to the shopping cart, and then 45 minutes have passed without any further activity, so we can derive a Shopper abandons cart event. On the right-hand side, Shopper B has added a product to the shopping cart and checked out within 20 minutes, not abandoning the cart.

For this chapter, we will opt for the second approach and write our Shopper abandons cart events to a new Kafka topic, called derived-events-ch05. Figure 5.2 shows our new stream processing job, reading events from the raw-events-ch05 topic, and writing new events to a derived-events-ch05 topic. Figure 5.2 Our abandoned shopping cart detector consumes events from the raw-events-ch05 topic in Kafka and generates new Shopper abandons cart events to write back to a new Kafka topic, called derived-events-ch05.

Modeling our new events Remember back in chapter 2, when we introduced our first e-commerce-related event for Nile, Shopper views product? Drawing on our standard definition of an event as subject-verb-object,

Shopper adds item to cart—The shopper adds one of those products to the shopping basket. A product is added to the basket with a quantity of one or more attached. ·   Shopper places order—The shopper checks out, paying for the items in the shopping basket. ·   Shopper abandons cart—The derived event itself, representing the act of cart abandonment by the shopper.

Figure 5.3 Our shopper (again, subject) adds (verb) an item, consisting of a product and its quantity (direct object) to the shopping cart (indirect, aka prepositional, object) at a given time (context).

Putting it all together, we get the event drawn in figure 5.4. 

Now we come to our derived event. By derived, we mean an event that we are generating ourselves in our stream processing application, as opposed to an incoming raw event that we are simply consuming. This event looks like this: ·   Subject: Shopper ·   Verb: Abandons ·   Direct object: Cart (consisting of multiple items, each of a product and quantity) ·   Context: Timestamp of this event Our fourth and final event for this chapter is illustrated in figure 5.5. 

5.3   Stateful stream processing To detect abandoned shopping carts for Nile, we have to do some stream processing on the incoming events. Clearly, it’s a little more complex than chapter 3, where we were able to work on only a single event at a time: our algorithm expects us to understand the flow of multiple events in a sequence over time. The key building block for processing multiple events like this is state.

Figure 5.7 depicts these three options. Regardless of the specific approach, the important thing to understand is that processing multiple events needs state.

Unfortunately for our purposes, a continuous event stream is unterminated, meaning that it has no end (and possibly no beginning either), so the first challenge in multiple-event processing is to come up with a sensible way of bounding a continuous event stream. We typically solve this problem by applying a processing window to our event stream. At its simplest, processing on a continuous event stream can be put into discrete windows by using a timer, or heartbeat, function that runs at a regular interval. This timer function can contain custom code to apply whatever window or windows makes sense for the use case. Figure 5.8 illustrates this idea of slicing an unending event stream into specific windows for further processing. Figure 5.8 A stream processing framework typically applies windowing to a continuous (unterminated) event stream to process it and generate meaningful outputs.

All this talk of processing multiple events, maintaining state, and creating stream windows probably sounds like a lot of work! To solve these and other problems in a repeatable, reliable way, a handful of stream processing frameworks have emerged.

State management—State is a key building block for processing multiple events at a time. Stream processing frameworks give you the option of storing state in some or all of the following: in-memory, in a local filesystem, or in a dedicated key-value store such as RocksDB or Redis.

Stream windowing—As described previously, a stream processing framework provides one or more ways of expressing a bounded window for the event processing. This is typically time-based, although sometimes it can be based on a count of events instead.

Delivery guarantees—All stream processing frameworks pessimistically track their progress, to ensure that every event is processed at least once. Some of these systems go further, adding in transactional guarantees to ensure that each event is processed only exactly once.

Task distribution—A high-volume event stream consists of multiple Kafka topics or Amazon Kinesis streams, and requires multiple instances of the job, or tasks, to process it. Most stream processing frameworks are designed to be run on a sophisticated third-party scheduler such as Apache Mesos or Apache Hadoop YARN;

Table 5.1 A nonexhaustive list of stream processing frameworks       Capability       Storm       Samza       Spark Streaming       Kafka Streams       Flink    State management      In-memory, Redis      In-memory, RocksDB      In-memory, filesystem  

Another great Manning book to dig into Kafka Streams is Kafka Streams in Action by William P. Bejeck Jr.

APACHE FLINK Apache Flink is a relative newcomer but rapidly emerging as a credible challenger to Spark and Spark Streaming.

mentioned at the beginning of this chapter, we are going to use Apache Samza to build our abandoned cart detector. Samza is a full-blown stream processing framework, but unlike the other tools we’ve introduced, Samza makes no effort to abstract or otherwise hide away the stateful nature of the event processing involved: with Samza, you will be working directly with a simple key-value data store to keep track of events that you have already processed.

What is a key-value store? It’s a super-simple database in which you store a single value (an array of bytes) against an individual unique key (also represented by an array of bytes); the key and the value can be any value that you like. A key-value store is a crude tool, but an effective one. It will give us the state we need to track shopper behavior across multiple events. Samza uses RocksDB, an embeddable persistent (versus in-memory only) key-value store, created by Facebook.

Samza has other tricks up its sleeves. The core of a Samza job consists of just two functions: ·   process()—Called for each incoming event ·   window()—Called on a regular, configurable interval Both functions have access to the key-value store, plus any mutable state defined in the job; this lets them effectively communicate with each other.

A recap: we are writing a stateful stream-processing job that will search the incoming stream, looking for a specific pattern of events, and when that pattern is found, the job will emit a new event, Shopper abandons cart.

With a key-value store, it is completely up to us how we design our keyspace—the

<shopper>-ts should be kept up-to-date with the timestamp

<shopper>-cart should be kept up-to-date with the current contents of the shopper’s cart,

Putting this together, we have two events that we are interested in for our process() function: ·   Shopper adds item to cart—For

Shopper places order—For telling us to “reset” tracking for this user

For our window() function, we will scan through the whole key-value store, looking for shoppers who were last active more than 30 minutes ago, based on the <shopper>-ts values.

Figure 5.9 Our process() function parses incoming events from Nile’s shoppers and updates the Samza key-value store to track the shoppers’ behavior. The window() function then runs regularly to scan the key-value store and identify shoppers who have abandoned their carts. A Shopper abandons cart event is then emitted for those shoppers.

There’s a lot to take in with this Samza job: let’s briefly review how our new job’s process() and window() functions work. To start with process(): ·   We are interested in only Shopper places order and Shopper adds item to cart events. ·   When a Shopper adds item to cart, we update a copy of their cart stored in our key-value store and update our shopper’s last active timestamp. ·   When a Shopper places order, we delete all state about our shopper from the key-value store. Our process() function is responsible for keeping a copy of each shopper’s cart up-to-date based on their add-to-cart events; it is also responsible for understanding how recently the user added something to their cart.

Now let’s briefly recap the window() function: ·   Every 30 seconds, we scan the whole key-value store, looking for shoppers who were last active more than 30 minutes ago. ·   We generate a Shopper abandons cart event for each shopper we find, detailing the contents of their shopping cart as recorded in our key-value store. ·   We send each Shopper abandons cart event to our outbound Kafka stream. ·   We delete from the key-value all values for the shoppers who just abandoned their carts.

5.5   Running our Samza job Although Samza now supports being embedded into a regular JVM application (like Kafka Streams), the more common way of running a Samza job is via YARN. Unless you have previously worked with Hadoop, it’s unlikely that you will have encountered YARN before—so we will introduce YARN briefly before getting our job running on

YARN stands for Yet Another Resource Negotiator—an uncomplimentary backronym for an important piece of technology. YARN is a software system that evolved out of Hadoop 1. The biggest difference between Hadoop 1 and Hadoop 2 is the separation of the cluster management responsibility into the YARN subproject. YARN is deployed onto a Hadoop cluster to allocate resources to YARN-aware applications effectively. It has three core components: ·   ResourceManager—The central “brain” that tracks servers in the Hadoop cluster and jobs running on those servers, and allocates compute resources to these jobs ·   NodeManager—Runs on every server in the Hadoop cluster, monitoring the jobs and reporting back to the ResourceManager ·   ApplicationMaster—Runs alongside each application and negotiates the required resources from the ResourceManager, and works with the NodeManager to execute and monitor each task

Summary ·   Processing multiple events from a stream requires state. This state allows the app to “remember” important attributes about individual events, across many events. ·   State for event stream processing can be kept in-memory (transient), stored locally on the processing instance, or written to a remote database. A stream processing framework can help us to manage this process. ·   Stream processing frameworks also help us to apply time windows to our streams, have delivery guarantees for our events, distribute our stream processing across multiple servers, and tolerate failures

Popular stream-processing frameworks include Apache Storm, Apache Samza, Spark Streaming, Kafka Streams, and Apache Flink.

Figure 6.1 All of the NCX-10 machines in Plum’s factories should emit a standard health-check event to a Kinesis stream every second.

Across Plum’s 10 factories, we have 10,000 machines. With each machine emitting a health check every second, that’s 36 million health checks landing in the Kinesis stream each hour, a substantial first event source for Plum’s unified log.

what health-check information we can retrieve from the NCX-10 machines. We discover a few interesting data points: ·   The name of the factory that the machine is installed in. ·   The machine’s serial number, which is a string. ·   The machine’s current status, which can be one of STARTING, RUNNING, or SHUTTING_DOWN. ·   The time when the machine was last started, which is a Unix timestamp accurate to milliseconds. ·   The machine’s current temperature, in Celsius. ·   Whether or not the machine has been earmarked as being end-of-life, meaning that it should be scrapped soon. ·   If the factory has multiple floors, the machine knows which floor it is situated on.

"factory": "Factory A",   "serialNumber": "EU3571",   "status": "RUNNING",   "lastStartedAt": 1474141826926,   "temperature": 34.56,   "endOfLife": false,   "floorNumber": 2    } The plant engineers squint at the preceding example JSON and confirm that this is pretty much everything that an NCX-10 machine can usefully emit.

A unified log is a fundamentally decoupled architecture: consumers and producers of event streams have no particular knowledge of each other.

The consumers of the event streams within our unified log need exactly the same kind of guarantees as the buyers of Plum’s widgets. We can give them these guarantees by using schemas for the events we are storing in our unified log. Schema is the Greek word for shape, and an event schema is simply that: a declaration that some set of events in our unified log will follow a predefined shape.

In the absence of formal integrations between systems, our event schemas are the closest we come to a contract between the system generating an event stream and the systems consuming that stream. Figure 6.2 illustrates this contract. Figure 6.2 The producer of the event agrees to the schema of the event with the initial known consumer of the event. This acts as a contract between both parties. Then, in the stream, the producing app creates events using the agreed-upon schema, and the consuming app can happily read those events, safe in the knowledge that the events will conform to the schema.

But how do we represent our event schemas? The good news is that we have a choice of a variety of schema technologies, often called data serialization systems,

The events we created for online retailer Nile certainly each had a shape, but that shape was not formally documented in a machine-readable way; we could say that Nile’s events had implicit schemas rather than explicit.

A variety of schema technologies, also known as data serialization systems,

Avro has a declarative JSON-based schema language for describing data types, as well as an alternative language, called Avro IDL, which is more C-like.

Apache Thrift (https://thrift.apache.org\/) is pitched as a framework for cross-language services development. As part of this, it includes a definition language that allows you to define data types as well as service interfaces.

JSON Schema (https://json-schema.org/) is slightly different from the other schema technologies described here. JSON Schema is a declarative language, itself written in JSON, for describing your JSON data’s format; it is easily written by humans

Protocol buffers (https://developers.google.com/protocol-buffers/) are a schema technology and data serialization mechanism from Google, currently on its third major version

If you have existing batch- or stream-processing systems that make heavy use of JSON, or you expect a lot of event authoring to be done by developers who prefer JSON, consider JSON Schema.

From our colleagues on the factory floor, we know what data points our health-check events need to contain, and from the brief review in the previous section, we know that we want to use Avro as our schema technology at Plum.

which correspond to the seven data points identified for a health-check event: ·   The name of the factory in which the machine is installed is of type string. ·   The machine’s serial number is another string. ·   The machine’s current status is an Avro enum (short for enumeration), which can be one of STARTING, RUNNING, or SHUTTING_DOWN. An enum is a complex type, so we need to provide it with a namespace (plum.avro) and a name (StatusEnum). ·   The time when the machine was last started, which is a Unix timestamp accurate to milliseconds. This is stored as a long, but Avro also lets us specify a logical type for the field, which gives a hint as to how a parser should handle the underlying type. ·   The machine’s current temperature, in Celsius, is a float. ·   Whether or not the machine has been earmarked as being end-of-life is a boolean. ·   The floor number is stored as a union type of an int or a null. If the factory does not have multiple floors, this field will be set to null. Otherwise, it will be an integer.

Remember that Avro has two representations, a human-readable JSON encoding and a more efficient binary encoding.

SELF-DESCRIBING EVENTS (HETEROGENEOUS OR HOMOGENEOUS STREAMS) In this approach, we again mix many event types into a single stream. The difference this time is that each event has a simple metadata “envelope” attached to it, which tells any consuming application which schema was used to serialize the event. We can call these events self-describing events, because the event carries with it at all times the information about what schema this instance is associated with. Figure 6.5 depicts this approach. Figure 6.5 In this Kafka topic, we can see three self-describing events: a health check, a machine restart, and another event. Each event consists of a schema describing the event, plus the event itself.

Working with self-describing events is a two-step process: 1. Parse the event’s metadata envelope to retrieve the identifier for the event’s schema. 2. Parse the event’s data portion against the identified schema.

We can write these events into a single heterogeneous stream, but equally we can then “shred” that stream into substreams of single or associated event types.

Because the event’s schema travels with the event itself, we can send the event on anywhere without losing the necessary metadata to parse the event. This is visualized in figure 6.6. Figure 6.6 With self-describing events, we can switch between heterogeneous streams and homogeneous streams at will, because a reference to the schema travels with the event. In this example, we have a “splitter app” separating health checks and machine restarts into dedicated Kafka topics.

An important thing to note: with self-describing events, we are always talking about adding some kind of pointer to the event’s original schema to the event. This pointer is a reference rather than the schema itself, not least because the schema itself is often huge—larger even than the event payload!

{ "name": "SelfDescribing",   "namespace": "plum.avro",   "type": "record",   "fields": [     { "name": "schema", "type": "string" },     { "name": "data", "type": "bytes" }   ] } You can see two fields in the SelfDescribing record: ·   schema is a string to identify the schema for the given event. ·   data is a sequence of 8-bit unsigned bytes, itself containing the Avro binary representation of the given event.

A better implementation of self-describing Avro for the JSON format would look something like this: { "schema": "com.plum/ncx10-health-check/1",   "data": {     "factory": "Factory A",     "serialNumber": "EU3571",     "status": "RUNNING",     "lastStartedAt": 1474141826926,     "temperature": 34.56,     "endOfLife": false,     "floorNumber": { "int": 2 }   }    }

1. Parse the event initially as JSON to extract the schema string. 2. Retrieve the Avro schema per the schema string. 3. Use the schema string to retrieve the JSON node for the event’s data. 4. Deserialize the JSON data node into a Check POJO object by using the retrieved Avro schema.

Figure 6.7 We have two Kafka applications that want to process health-check events. Both of these apps contain a definition of the health-check event in Avro; there is no central source of truth for this schema. This copy-and-paste approach is a bad idea, because we don’t have a single source of truth for the definition of an NCX-10 health-check event within Plum.

Plum needs a single source of truth for all of our schemas—a single location that registers our schemas, and that all teams can access to understand the definition of each schema.

Figure 6.8 visualizes the schema registry for Plum, containing three event schemas. Figure 6.8 Plum now has a schema registry that contains the master copy of the schema for each type of event. All consuming and producing apps should use these master copies. At its simplest, a schema registry can be just a shared folder, perhaps on S3, HDFS, or NFS. The schema syntax we defined earlier maps nicely onto a folder structure, here in Amazon S3: s3://plum-schemas/com.plum/ncx10-health-check/1 The file at the preceding path would be the Avro definition file for version 1 of the NCX-10 health-check event.

A unified log is a decoupled architecture: consumers and producers of event streams have no particular knowledge of each other. ·   The contract between event consumers and producers is represented by the schema of each event. ·   Schema technologies we can use include JSON Schema, Apache Avro, Thrift, and protocol buffers. Avro is a good choice. ·   We can define our event schemas by using Avro’s JSON-based schema syntax, and then use code-generation to build Java bindings (think POJOs) to the schemas.

In Kinesis, the trim horizon is set to 24 hours and can be increased up to 1 week (or 168 hours). After that cutoff, older events are trimmed—deleted from the stream forever.

Figure 7.1 The Kinesis trim horizon means that events in our stream are available for processing for only 24 hours after first being written to the stream.

“If data will be trimmed after some hours or days, let’s just make sure we do all of our processing before that time window is up!”

Unfortunately, this approach has key shortcomings; we could call these the Three Rs: ·   Resilience ·   Reprocessing ·   Refinement

7.1.1   Resilience We want our unified log processing to be as resilient in the face of failure as possible. If we have Kinesis or Kafka set to delete events forever after 24 hours, that makes our event pipeline much more fragile: we must fix any processing failures before

7.1.1   Resilience We want our unified log processing to be as resilient in the face of failure as possible. If we have Kinesis or Kafka set to delete events forever after 24 hours, that makes our event pipeline much more fragile: we must fix any processing failures before those events are gone from our unified log forever.

Because we cannot foresee and fix all the various things that could fail in our job, it becomes important that we have a robust backup of our incoming events. We can then use this backup to recover from any kind of stream processing failure at our own speed.

7.1.2   Reprocessing In chapter 5, we wrote a stream processing job in Samza that detected abandoned shopping carts after 30 minutes of customer inactivity. This worked well, but what if we have a nagging feeling that a different definition of cart abandonment might suit our business better? If we had all of our events stored somewhere safe, we could apply multiple different cart abandonment algorithms to that event archive, review the results, and then port the most promising algorithms into our stream processing job.

7.1.3   Refinement Assuming that the calculations and aggregations in our stream processing job are bug-free, just how accurate will our job’s results be?

good example, explored in Big Data, is calculating unique visitors to a website. Calculating unique visitors (COUNT DISTINCT in SQL) can be challenging because the metric is not additive—for example, you cannot calculate the number of unique visitors to a website in a month by adding together the unique visitor numbers for the constituent weeks. Because accurate uniqueness counts are computationally expensive, often we will choose an approximate algorithm such as HyperLogLog for our stream processing.

If a unified log pipeline duplicates events, this is a strong rationale for performing additional refinement, potentially using a technology that supports exactly-once processing. It is worth noting, though, that as of release 0.11, Apache Kafka also supports exactly-once delivery semantics.

Figure 7.5 By archiving as far upstream as possible, we insulate our archiving process from any failures that occur downstream of the raw event stream.

Event validation and enrichment can be a costly process, so why insist on archiving the rawest events? Again, it comes down to the three Rs: ·   Resilience—By archiving as upstream as possible, we are guaranteeing that there are no intermediate stream-processing jobs that could break and thus cause our archiving to fail. ·   Reprocessing—We may want to reprocess any part of our event pipeline—yes, even the initial validation and enrichment jobs. By having the rawest events archived, we should be able to reprocess anything downstream. ·   Refinement—Any refinement process (such as a Hadoop or Spark batch-processing job) should start from the exact same input events as the stream processing job that it aims to refine.

We need to archive our event stream to permanent file storage that has the following characteristics: ·   Is robust, because we don’t want to learn later that parts of the archive have been lost ·   Makes it easy for data processing frameworks such as Hadoop or Spark to quickly load the archived events for further processing or refinement

Distributed filesystem       Hosted?       API       Description    Amazon Simple Storage Service (S3)      Yes      HTTP      A hosted file storage service, part of Amazon Web Services.      Azure Blob Storage      Yes      HTTP      A hosted unstructured data storage service, part of Microsoft Azure.      Google Cloud Storage      Yes      HTTP      A hosted object storage service, part of Google Cloud Platform.      Hadoop Distributed File System (HDFS)      No      Java, Thrift      A distributed filesystem written in Java for the Hadoop framework.      OpenStack Swift      No      HTTP      A distributed, highly available, eventually consistent object store.      Riak Cloud Storage (CS)      No      HTTP      Built on the Riak database. API compatible with Amazon S3.      Tachyon      No      Java, Thrift      A memory-centric storage system optimized for Spark and Hadoop processing. Implements the HDFS interface.  

7.2.3   How to archive The fundamentals of archiving events from our unified log into our permanent file storage are straightforward. We need a stream consumer that does the following: ·   Reads from each shard or topic in our stream ·   Batches a sensible number of events into a file that is optimized for subsequent data processing ·   Writes each file to our distributed filesystem of choice

Secor      Kafka      S3      Pinterest      A service for persisting Kakfa topics to S3 as Hadoop SequenceFiles.  

Let’s return to Nile, our online retailer from chapter 5. Remember that Nile’s shoppers generate three types of events, all of which are collected by Nile and written to their Apache Kafka unified log. The three event types are as follows: ·   Shopper views product ·   Shopper adds item to cart ·   Shopper places order Alongside the existing stream processing we are doing on these events, Nile wants to archive all of these events to Amazon S3. Figure 7.6 sets out the desired end-to-end architecture. Figure 7.6 Alongside Nile’s three existing stream-processing applications, we will be adding a fourth application, which archives the raw event stream to Amazon S3.

7.4   Batch processing our archive Now that we have our raw events safely archived in Amazon S3, we can use a batch processing framework to process these events in any way that makes sense to Nile.

The fundamental difference between batch processing frameworks and stream processing frameworks relates to the way in which they ingest data. Batch processing frameworks expect to be run against a terminated set of records, unlike the unbounded event stream (or streams) that a stream-processing framework reads. Figure 7.9 illustrates a batch processing framework. 

Figure 7.9 A batch processing framework has processed four distinct batches of events, each belonging to a different day of the week. The batch processing framework runs at 3 a.m. daily, ingests the data for the prior day from storage, and writes its outputs back to storage at the end of its run.

By way of comparison, figure 7.10 illustrates how a stream processing framework works on an unterminated stream of events. Figure 7.10 A stream processing framework doesn’t distinguish any breaks in the incoming event stream. Monday through Thursday’s data exists as one unbounded stream, likely with overlap due to late-arriving events.

Table 7.3 lists the major distributed batch-processing frameworks. Of these, Apache Hadoop and, increasingly, Apache Spark are far more widely used than Disco or Apache Flink.

What do we mean when we say that these frameworks are distributed? Simply put, we mean that they have a master-slave architecture: ·   The master supervises the slaves and parcels out units of work to the slaves. ·   The slaves (sometimes called workers) receive the units of work, perform them, and provide status updates to the master. Figure 7.11 represents this architecture. This distribution allows processing to scale horizontally, by adding more slaves. Figure 7.11 In a distributed data-processing architecture, the master supervises a set of slaves, allocating them units of work from the batch processing job.

The analytics team at Nile wants a report on the lifetime behavior of every Nile shopper: ·   How many items has each shopper added to their basket, and what is the total value of all items added to basket? ·   Similarly, how many orders has each shopper placed, and what is the total value of each shopper’s orders? Figure 7.12 shows a sample report. Figure 7.12 For each shopper, the Nile analytics team wants to know the volume and value of items added to basket, and the volume and value of orders placed. With this data, the value of abandoned carts is easily calculated.

we imagine that the Nile analytics team came up with this report six months into us operating our event archive, we can see that this is a classic piece of reprocessing: Nile wants us to apply new aggregations retrospectively to the full event history.

Before we write a line of code, let’s come up with the algorithms required to produce this report. We will use SQL-esque syntax to describe the algorithms. Here’s an algorithm for the shoppers’ add-to-basket activity: GROUP BY shopper_id WHERE event_type IS add_to_basket   items = SUM(item.quantity)   value = SUM(item.quantity * item.price)

And that’s it. Now that you know what calculations we want to perform, we are ready to pick a batch processing framework and write them.

And that completes our experiments writing a Spark job at the Scala console! You have seen that we can build a sophisticated report for the Nile analytics team by using Spark SQL. But running this inside a Scala console isn’t a realistic option for the long-term, so in the next section we will look briefly at operationalizing this code by using Amazon’s Elastic MapReduce platform.

Setting up and maintaining a cluster of servers for batch processing is a major effort, and not everybody has the need or budget for an always-running (persistent) cluster. For example, if Nile wants only a daily refresh of the shopper spend analysis, we could easily achieve this with a temporary (transient) cluster that spins up at dawn each day, runs the job, writes the results to Amazon S3, and shuts down. Various data-processing-as-as-a-service offerings have emerged to meet these requirements, including Amazon Elastic MapReduce (EMR), Quobole, and Databricks Cloud.

7.5   Summary ·   A unified log such as Apache Kafka or Amazon Kinesis is not designed as a long-term store of events. Kinesis has a hard limit, or trim horizon, of a maximum of 168 hours, after which records are culled from a stream. ·   Archiving our events into long-term storage enables three important requirements: reprocessing, robustness, and refinement. ·   Event reprocessing from the archive is necessary when additional analytics requirements are identified, or bugs are found in existing processing. ·   Archiving all raw events gives us a more robust pipeline. If our stream processing fails, we have not lost our event stream. ·   An event archive can be used to refine our stream processing. We can improve the accuracy of our processing in the face of late-arriving data, deliberate approximations we applied for performance reasons, or inherent limitations of our stream processing framework. ·   We should archive our rawest events (as upstream in our topology as possible) to a distributed filesystem such as HDFS or Amazon S3, using a tool such as Pinterest’s Secor, Snowplow’s kinesis-s3, or Confluent’s Connect S3. ·   Once we have our events archived in a distributed filesystem, we can perform processing on those events by using a batch processing framework such as Apache Hadoop or Apache Spark. ·   Using a framework such as Spark, we can write and test an event processing job interactively from the Scala console or REPL. ·   We can package our Spark code as a fat jar and run it in a noninteractive fashion on a hosted batch-processing platform such as Amazon Elastic MapReduce (EMR).

Unix is designed around the idea that failures will happen. Any process that runs in a Unix shell will return an exit code when it finishes. The convention is to return zero for success, and an integer value higher than zero in the case of failure.

This exit, or return, code isn’t the only communication channel available to a Unix program; each program also has access to three standard streams (aka I/O file descriptors): stdin, stdout, and stderr. Table 8.1 provides the properties of these three streams. Table 8.1 The three standard streams supported by Unix programs       Short name       Long name       File descriptor       Description      stdin    Standard in      0      The input stream of data going into a Unix program        stdout    Standard out      1      The output stream where a Unix program writes data related to successful operation        stderr    Standard error      2      The output stream where a Unix program writes data related to failed operation  

Figure 8.1 A Unix program reads from standard in and can respond with exit codes and output streams along a happy path or a failure path.

Figure 8.2 Three Unix programs form a single Unix pipeline by piping the stdout of the first program into the stdin of the next program. Many shells support an option to pipe both stdout and stderr into stdin, but in both cases the program’s exit codes are ignored. If we do want to fail fast, we have to put our commands in a shell script that uses the set –e option, which will terminate the script as soon as any command within the script returns a nonzero error code. If you run the code in the following listing, you will see the following output; notice that the second echo in the shell script is never reached: $ ./fail-fast.bash; echo $? s1 1 Listing 8.1 fail-fast.bash #!/bin/bash set -e   echo "s1" false echo "s3"    #A

#A This line is never reached.

Java’s built-in failure handling is broadly designed around two failure scenarios: ·   We have an unrecoverable bug and we want to terminate the program. ·   We have a potentially recoverable issue and we want to try to recover from it (and if we can’t recover from it, we terminate the program).

But what if we cannot recover from a failure but don’t want to terminate our overall program?

A stream processing job in our unified log, tasked with enriching many millions of individual events. The enrichment of each incoming event is a unit of work.

In each of these scenarios, the programmer may prefer to route the unrecoverable unit of work to a failure path rather than terminate the whole program, as shown in figure 8.3. Figure 8.3 Our Java program is processing four items of input, one of which is somehow corrupted. Our Java program performs the unit of work on each of the four inputs: three are successfully output along the happy path, but the corrupted input throws an error and ends up on the failure path.

As with many other languages, Java does not have any built-in tools for routing failing units of work to a failure path. In situations like these, a Java programmer will often fall back to simply logging the failure as an error and skipping to the next unit of work.

many experienced programmers criticize the concept of exceptions for a similar reason: they create a secondary control flow in a program, one that exists outside the standard imperative flow, and thus is difficult to reason about.

a simple pattern for unified log processing that accounts for the failure path as well as the happy path.

We should terminate our job only if one of the following occurs: ·   We encounter an unrecoverable error during the initialization phase of our job. ·   We encounter a novel error while processing a unit of work inside our job. A novel error means one that we haven’t seen before: a Rumsfeld-esque unknown unknown. Although it might be tempting to keep processing in this case to minimize disruption, terminating the job forces us to evaluate any novel error as soon as we encounter it. We can then determine how this new error should be handled in the future—for example, can it be recovered from without failing the unit of work?

Next, how do we handle an unrecoverable but not-unexpected error within a unit of work? Here there is no way around it: we have to move this unit of work into our failure path, but making sure to follow a few important rules: ·   Our failure path must not be out-of-band. We don’t need to rely on third-party logging tools; we are implementing a unified log, so let’s use it! ·   Entries in our failure path must contain the reason or reasons for the failure in a well-structured form that both humans and machines can read. ·   Entries in our failure path must contain the original input data (for example, the processed event), so that the unit of work can potentially be replayed if and when the underlying issue can be fixed.

Let’s make this a little more concrete with the example of a simple stream-processing job that is enriching individual events, as shown in figure 8.4. Figure 8.4 Our enrichment job processes events from the input event stream, writes enriched events to our happy path event stream, and writes input events that failed enrichment, plus the reasons why they failed enrichment, to our failure path event stream.

Does the flow in figure 8.4 look familiar? It shares a lot in common with Unix’s concept of three standard streams: our stream processing app will read from one event stream, and write to two event streams, one for the happy path and the other for individual failures. At the same time, we have improved some of the failure-handling techniques of section 8.1: ·   We have done away with exit values. The success or failure of a given unit of work is reflected in whether output was written to the happy event stream or the failure event stream. ·   We have removed any ambiguity in the outputs. A unit of work results in output either to the happy stream or to the failure stream, never both, nor neither. Three input events mean a total of three output events. ·   We are using the same in-band tools to work with both our successes and our failures. Failures will end up as well-structured entries in one event stream; successes will end up as well-structured entries in the other event stream.

8.2.2   Modeling failures as events How could we describe our stream processing job’s failure to enrich an event read from an input stream? Perhaps something like this: At 12:24:07 in our production environment, SimpleEnrich v1 failed to enrich Inbound Event 428 because it failed JSON Schema validation.

·   Subject—In this case, our stream processing job, SimpleEnrich v1, is the entity carrying out the action of this event. ·   Verb—The action being done by the subject is, in this case, “failed to enrich.” ·   Direct object—The entity to which the action is being done is Inbound Event 428. ·   Timestamp—This tells us exactly when this failure occurred.

Figure 8.5 We are representing our enrichment as an event: the subject is the enrichment job itself, the verb is “failed to enrich,” and the direct object is the event we failed to enrich.

Figure 8.6 Our enrichment job reads six input events; three fail enrichment and are written out to our failure stream. We then feed those three failures into a cleaning job that attempts to fix the corrupted events. The job manages to fix two events, which are fed back into the original enrichment job.

Figure 8.7 We have composed a happy path by chaining together the happy output event stream of each stream processing job as the input stream of the next job.

Our retailer already has all customer orders available in a stream in its unified log, so the managers want us to write a stream processing job that does the following: ·   Reads customer orders from the incoming event stream ·   Converts all customer orders into euros ·   Writes the updated customer orders to a new event stream The currency conversion will be done using the live (current) exchange rate, to keep things simple. Our job can look up the exchange rate by making an API call to a third-party service called Open Exchange Rates (https://openexchangerates.org/).

This sounds simple. What could go wrong with the operation of our job? In fact, a few things: ·   Perhaps Open Exchange Rates is having an unplanned outage or planned downtime to facilitate an upgrade. ·   Perhaps test data made it into the production flow, and the customer order is in a currency other than one of the three allowed ones. ·   Perhaps a hacker has managed to transact an order with a non-numeric order value, getting themselves a huge flat screen TV for €l33t.

will need to plan for failure: in addition to the output stream of orders in our base currency—our happy path—we will need a second output stream to report our currency conversion failures—our failure path. Figure 8.8 illustrates the overall job flow. Figure 8.8 Our input event stream contains two customer order events. The valid one is successfully converted into euros and written to our happy stream. The second event is invalid and is written to our failure stream; our failure event records the failures encountered as well as the original event.

Figure 8.9 For the happy path, our function returns a Currency boxed inside a Success. For the failure path, our function returns an error String boxed inside a Failure. Success and Failure are the two modes of the scalaz Validation type.

Figure 8.9 For the happy path, our function returns a Currency boxed inside a Success. For the failure path, our function returns an error String boxed inside a Failure. Success and Failure are the two modes of the scalaz Validation type. This switch from using exceptions to using scalaz’s Validation to box either success or failure might not seem like a big change, but it’s going to be our key building block for working with failure in Scala.

Figure 8.9 For the happy path, our function returns a Currency boxed inside a Success. For the failure path, our function returns an error String boxed inside a Failure. Success and Failure are the two modes of the scalaz Validation type. This switch from using exceptions to using scalaz’s Validation to box either success or failure might not seem like a big change, but it’s going to be our key building block for working with failure

Figure 8.11 The idealized and pragmatic happy paths vary based on how late the exchange rate lookup is performed. In the pragmatic happy path, we validate as much as we can before attempting the exchange rate lookup. The pragmatic happy path is so-called because looking up a currency from a third-party service’s API over HTTP is an expensive operation, whereas validating our order amount is relatively cheap.

Figure 8.12 Processing of our raw event proceeds down the happy track, but a failure at any given step will switch us onto the failure track. When we are on the failure track, we stay there, bypassing any further happy-track processing.

What we want to do is connect the Success output of one to the input of the next, but somehow bypass the second function in case of a Failure output....There is a great analogy for doing this—something you are probably already familiar with. Railways! Railways have switches (“points” in the UK) for directing trains onto a different track. We can think of these “Success/Failure” functions as railway switches....We will have a series of black-box functions that appear to be straddling a two-track railway, each function processing data and passing it down the track to the next function....Note that once we get on the failure path, we never (normally) get back onto the happy path. We just bypass the rest of the functions until we reach the end.

The answer to this question lies in the flat in the name: flatMap flattens the two Validation containers into one. flatMap is the secret sauce of our railway-oriented approach: it allows us to chain multiple processing steps together into one stream processing job without accumulating another layer of Validation boxing at every stage. Figure 8.14 depicts the unworkable alternative. Figure 8.14 If our flatMap did not flatten, we would end up with a Matryoshka-doll-like collection of Validations inside other Validations. It would be extremely difficult to work with this nested type correctly.

Figure 8.15 This visualization of our convert function shows how our now-familiar Validation boxes interact first with a flatMap and then with a map. flatMap allows us to chain together multiple steps that each might result in Failure, without nesting Validations inside other Validations. Both flatMap and map are respecting the fail-fast requirement of the pipeline, with no further processing after a Failure has occurred.

Unix’s concept of three standard streams, one for input and one each for good and bad output, is a powerful idea that we can apply to our own unified log processing.

In the unified log, we can model failures as events themselves. These events should contain all of the causes of the failure, and should also contain the original event, to enable reprocessing in another stream-processing job.

Stream processing jobs should echo the Unix approach and write successes to one stream, and failures to another stream. This allows us to compose complex processing flows out of multiple jobs.

the command. A command is an order or instruction for a specific action to be performed in the future—each command, when executed, produces an event.

Working for Plum, our fictitious global consumer-electronics manufacturer, we will define a command intended to alert Plum maintenance engineers of overheating machines on the factory floor. We will represent this command in the Apache Avro schema language and use the Kafka producer script from chapter 2 to publish those alert commands to our unified log. We will then write a simple command-executor app by using the Kafka Java client’s producer and consumer capabilities. Our command executor will read the Kafka topic containing our alert commands and send an email to the appropriate support engineer for each alert; we will use Rackspace’s Mailgun transactional email service to send the emails.

A command is an order or instruction for a specific action to be performed in the future. Here are some example commands: ·   “Order a pizza, Dad.” ·   Tell my boss that I quit. ·   “Renew our household insurance, Jackie.” If an event is a record of an occurrence in the past, then a command is the expression of intent that a new event will occur in the future:

Figure 9.1 A decision produces a command—an order or instruction to do something specific. If that command is then executed, we can record an event as having occurred.

You can see that there is a symbiotic relationship between commands and events: strictly speaking, without commands, we would have no events.

Figure 9.2 Within our unified log, a source event stream is used to drive a decision-making app, which emits commands to a second stream. A command-execution app reads that stream of commands, executes the commands, and then emits a stream of events recording the command executions.

The decision-making job—This will parse the event stream of NCX-10 health checks to detect signs of machines overheating. If these are detected, this job will emit a command to alert a maintenance engineer. ·   The command-execution job—This will read the event stream containing commands to alert a maintenance engineer, and will send each of these alerts to the engineer. Figure 9.4 depicts these two jobs and the Kafka streams between them. Figure 9.4 Within Plum’s unified log, a stream of health-check events is read by a decision-making app to detect overheating machines and emit alert commands to a second stream. A command-execution app reads the stream of commands and sends emails to alert the maintenance engineers.

We know what this command needs to do—alert a maintenance engineer—but how do we decide what fields to put in our command? Like a tasty muffin, a good command should have these characteristics: ·   Shrink-wrapped—The command must contain everything that could possibly be required to execute the command; the executor should not need to look up additional information to execute the command. ·   Fully baked—The command should define exactly the action for the executor to perform; we should not have to add business logic into the executor to turn each command into something actionable. Here is an example of a badly designed self-describing alert command for Plum: { "command": "alert_overheating_machine",   "recipient": "Suzie Smith",   "machine": "cz1-123",   "createdAt": 1460412544992    }

Here is a better version of an alert command for Plum: { "command" : "alert",     "notification": {     "summary": "Overheating machine",     "detail": "Machine cz1-123 may be overheating!",     "urgency": "MEDIUM"   },   "recipient": {     "name": "Suzie Smith",     "phone": "(541) 754-3010",     "email": "s.smith@plum.com"   },   "createdAt": 1460412544992 }

We could use lots of different stream-processing frameworks to execute our commands, but remember that command execution involves only two tasks: 1. Reading each command from the stream and executing it 2. Emitting an event to record that the command has been executed Figure 9.5 shows the specifics of these two tasks for Plum. Figure 9.5 The command-execution app for Plum will send an email to the support engineer via Mailgun and then emit an email_sent event to record that the alert has been executed.

Great! We have now implemented a command executor that can take incoming commands—in our case, alerts for a long-suffering Plum maintenance engineer—and convert them into emails to that engineer. Our command executor even emits an email_sent event to track that the action has been performed.

Figure 9.9 illustrates these three options. Figure 9.9 We could define one stream for each command type (option A), associate commands to priority-based streams (option B), or use one stream for all commands (option C).

Commands should be carefully modeled to ensure that they are shrink-wrapped and fully baked. This ensures that decision-making and command execution can stay loosely coupled, with a clear separation of concerns.

In other words: store first; ask questions later. Does this sound familiar? We’ve been here before, in chapter 7, where we archived all of our events to Amazon S3 and then wrote a simple Spark job to generate a simple analysis from those events. This was classic analytics-on-read: first we wrote our events to a storage target (S3), and only later did we perform the required analysis, when our Spark job read all of our events back from our S3 archive. An analytics-on-read implementation has three key parts: ·   A storage target to which the events will be written. We use the term storage target because it is more general than the term database. ·   A schema, encoding, or format in which the events should be written to the storage target. ·   A query engine or data processing framework to allow us to analyze the events as read from our storage target. Figure 10.1 illustrates these three components. Figure 10.1 For analytics-on-read, we first write events to our storage target in a predetermined format. Then when we have a query about our event stream, we run that query against our storage target and retrieve the results.

These requirements point to a much more operational analytical capability—one that is best served by analytics-on-write. Analytics-on-write is a four-step process: 1. Read our events from our event stream. 2. Analyze our events by using a stream processing framework. 3. Write the summarized output of our analysis to a storage target. 4. Serve the summarized output into real-time dashboards or reports. We call this analytics-on-write because we are performing the analysis portion of our work prior to writing to our storage target; you can think of this as early, or eager, analysis, whereas analytics-on-read is late, or lazy, analysis. Again, this approach should seem familiar; when we were using Apache Samza in part 1, we were practicing a form of analytics-on-write!

Figure 10.2 shows a simple example of analytics-on-write using a key-value store. Figure 10.2 With analytics-on-write, the analytics are performed in stream, typically in close to real time, and the outputs of the analytics are written to the storage target. Those outputs can then be served into dashboards and reports.

Figure 10.3 Our unified log feeds three discrete systems: our event archive, an analytics-on-read system, and an analytics-on-write system. Event archives were discussed in chapter 7.

Table 10.1 Comparing the main attributes of analytics-on-read to analytics-on-write       Analytics-on-read       Analytics-on-write    Predetermined storage format      Predetermined storage format      Flexible queries      Predetermined queries      High latency      Low latency      Support 10–100 users      Support 10,000s of users      Simple (for example, HDFS) or sophisticated (for example, HP Vertica) storage target      Simple storage target (for example, key-value store)      Sophisticated query engine or batch processing framework      Simple (for example, AWS Lambda) or sophisticated (for example, Apache Samza) stream processing framework  

The answer may not be what you are expecting: you should archive the rawest events you can, as far upstream in your event pipeline as possible. This early archiving is shown in figure 7.5, which builds on the unified log topology set out earlier in figure 7.3.

For familiarity’s sake, let’s say that we again stored our events in Amazon S3 as JSON, and we then wrote a variety of Apache Spark jobs to derive insights from the event archive.

1. Our delivery trucks and our drivers’ handheld computers emit events that are received by an event collector (most likely a simple web server). 2. The event collector writes these events to an event stream in Amazon Kinesis. 3. A stream processing job reads the stream of raw events and validates the events against their JSON schemas. 4. Events that fail validation are written to a second Kinesis stream, called the bad stream, for further investigation. 5. Events that pass validation are written to an Amazon S3 bucket, and partitioned into folders based on the event type. Phew—there is a lot to take in here, and you may be thinking that these steps are not relevant to us, because they take place upstream of our analytics. But a good analyst or data scientist will always take the time to understand the source-to-sink lineage of their event stream. We should be no different! Figure 10.6 illustrates this five-step event-archiving process. Figure 10.6 Events flow from our delivery trucks and drivers into an event collector. The event collector writes the events into a raw event stream in Kinesis. A stream processing job reads this stream, validates the events, and archives the valid events to Amazon S3; invalid events are written to a second stream.

few things to note before we continue: ·   The validation step is important, because it ensures that the archive of events in Amazon S3 consists only of well-formed JSON files that conform to JSON Schema. ·   In this chapter, we won’t concern ourselves further with the bad stream, but see chapter 8 for a more thorough exploration of happy versus failure paths. ·   The events are stored in Amazon S3 in uncompressed plain-text files, with each event’s JSON separated by a newline. This is called newline-delimited JSON (http://ndjson.org).

AWS refers to this as single-node versus multi-node—confusing because a single-node cluster still has a leader node and a compute node, but they simply reside on the same EC2 instance. On a multi-node cluster, the leader node is separate from the compute nodes.

The table is sparsely populated, meaning that columns will be empty if an event does not feature a given entity. Figure 10.10 depicts this approach. Figure 10.10 The fat-table approach records all event types in a single table, which has columns for each entity involved in the event. We refer to this table as “sparsely populated” because entity columns will be empty if a given event did not record that entity.

We can divide event stream analytics into analytics-on-read and analytics-on-write. ·   Analytics-on-read means “storing first, asking questions later.”

we will explore techniques for delivering these kinds of analytics, which broadly fall under the term analytics-on-write. Analytics-on-write has an up-front cost: we have to decide on the analysis we want to perform ahead of time and put this analysis live on our event stream. In return for this constraint, we get some benefits: our queries are low latency, can serve many simultaneous users, and are simple to operate.

Figure 11.11 First, we map the events from our microbatch into possible Rows, and filter out the Nones. We then group our Rows by the truck’s VIN and reduce each group to a single Row per truck by using a merge. Finally, we perform a conditional write for each Row against our table in DynamoDB.

Figure 11.12 When our Lambda function processes events for OOPS Truck 123 out of chronological order, it is important that a more recent data point in DynamoDB is not overwritten with a stale data point.

With analytics-on-write, we decide on the analysis we want to perform ahead of time, and then put this analysis live on our event stream. ·   Analytics-on-write is good for low-latency operational reporting and dashboards to support thousands of simultaneous users. Analytics-on-write is a complement to analytics-on-read, not a competitor. ·   To support the latency and access requirements, analytics-on-write systems typically lean heavily on horizontally scalable key-value stores such as Amazon DynamoDB.

We implemented a simple truck status dashboard for OOPS by using AWS Lambda as the stream processing framework reading OOPS events from Amazon Kinesis. ·   To reduce the number of calls to DynamoDB, we implemented a local map-reduce on the microbatch of events received by the Lambda function, reducing this batch of 100 events down to the minimal number of potential updates to DynamoDB. ·   AWS Lambda is essentially stateless, so we used DynamoDB’s conditional writes to ensure that we were always updating our DynamoDB table with the latest data points relating to each OOPS truck.
